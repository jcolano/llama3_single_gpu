{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "# Updated by Juan Olano 4/18/2024 \n",
    "# Simplified the code to adapt it for a single GPU. (https://github.com/jcolano/llama3_single_gpu.git)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set environment variables for distributed training\n",
    "os.environ['MASTER_ADDR'] = 'localhost'  # Adjust as necessary for your network setup\n",
    "os.environ['MASTER_PORT'] = '12355'      # Ensure this port is free on your network\n",
    "os.environ['RANK'] = '0'                 # '0' for the master process, '1' to N-1 for other processes\n",
    "os.environ['WORLD_SIZE'] = '1'           # Set to the number of processes involved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "        seed: int = 1,\n",
    "    ) -> \"Llama\":\n",
    "        \"\"\"\n",
    "        Build a Llama instance by initializing and loading a model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir (str): Path to the directory containing checkpoint files.\n",
    "            tokenizer_path (str): Path to the tokenizer file.\n",
    "            max_seq_len (int): Maximum sequence length for input text.\n",
    "            max_batch_size (int): Maximum batch size for inference.\n",
    "            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n",
    "                If not provided, it's determined from the environment. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Llama: An instance of the Llama class with the loaded model and tokenizer.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If there are no checkpoint files in the specified directory,\n",
    "                or if the model parallel size does not match the number of checkpoint files.\n",
    "\n",
    "        Note:\n",
    "            This method initializes the distributed process group, sets the device to CUDA,\n",
    "            and loads the pre-trained model and tokenizer.\n",
    "        \"\"\"\n",
    "\n",
    "        local_rank = 0 #int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "        # seed must be the same in all processes\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        start_time = time.time()\n",
    "        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "        ckpt_path = checkpoints[0]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )\n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        assert model_args.vocab_size == tokenizer.n_words\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        model = Transformer(model_args)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        \"\"\"\n",
    "        Generate text sequences based on provided prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n",
    "            max_gen_len (int): Maximum length of the generated text sequence.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n",
    "\n",
    "        Note:\n",
    "            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            logits = self.model.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                torch.isin(next_token, stop_tokens)\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to after eos tok if any\n",
    "            for stop_token in self.tokenizer.stop_tokens:\n",
    "                try:\n",
    "                    eos_idx = toks.index(stop_token)\n",
    "                    toks = toks[:eos_idx]\n",
    "                    probs = probs[:eos_idx] if logprobs else None\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        \"\"\"\n",
    "        Perform text completion for a list of prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): List of text prompts for completion.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n",
    "                If not provided, it's set to the model's maximum sequence length minus 1.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n",
    "\n",
    "        Note:\n",
    "            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Describe the advantages of using solar energy over fossil fuels.\",\n",
    "    \"Write a brief summary of the first chapter of 'Pride and Prejudice'.\",\n",
    "    \"Explain the process of photosynthesis in simple terms.\",\n",
    "    \"What are the top five things to consider when buying a new laptop?\",\n",
    "    \"Compose a short story about a detective solving a mystery in a futuristic city.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"../Meta-Llama-3-8B/\"\n",
    "tokenizer_path = \"../Meta-Llama-3-8B/tokenizer.model\"\n",
    "max_seq_len = 512\n",
    "max_batch_size = 8\n",
    "model_parallel_size = 0  # This should match your hardware and checkpoint strategy\n",
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 12.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Llama class\n",
    "llama_instance = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    "    model_parallel_size=model_parallel_size,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "prompt = \"Why do Chameleons change color?\"\n",
    "prompt = \"The Ottoman empire was\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the text_completion method with the prompt\n",
    "completions = llama_instance.text_completion(\n",
    "    prompts=[prompt],       # The prompts parameter expects a list of strings\n",
    "    temperature=0.6,        # Control the randomness of the output\n",
    "    top_p=0.9,              # Use nucleus sampling\n",
    "    max_gen_len=100,         # Maximum length of the generated completion\n",
    "    logprobs=False,         # Whether to return log probabilities of the generated tokens\n",
    "    echo=True               # Whether to include the original prompt in the output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Completion: <|begin_of_text|>The Ottoman empire was a Muslim state that ruled the Middle East and large parts of Europe and Africa from 1299 to 1923. It was founded by the Turks, a nomadic people from central Asia who had converted to Islam. The Ottoman empire was at its height in the 16th century, when it controlled much of the Middle East, North Africa, and the Balkans. The empire began to decline in the 17th century, and by the 19th century it was in a state of\n"
     ]
    }
   ],
   "source": [
    "# Print the completion for each prompt\n",
    "for completion in completions:\n",
    "    print(\"Generated Completion:\", completion['generation'])\n",
    "    # If logprobs were requested, you could also print them here\n",
    "    if 'logprobs' in completion:\n",
    "        print(\"Log probabilities:\", completion['logprobs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
